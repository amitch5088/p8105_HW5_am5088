---
title: "p8105_hw5_am5088"
author: "Anika Mitchell am5088"
output: github_document
---

load my necessary libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(tibble)
library(broom)

knitr::opts_chunk$set(
    fig.width = 6,
    fig.asp = .6,
    out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
    ggplot2.continuous.colour = "viridis",
    ggplot2.continuous.fil = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

I've gone ahead and loaded my necessary libraries and set up plotting for this problem (tidyverse), and will now start to develop a function to address the following question:

Suppose you put ð‘›people in a room, and want to know the probability that at least two people share a birthday. For simplicity, weâ€™ll assume there are no leap years (i.e. there are only 365 days) and that birthdays are uniformly distributed over the year (which is actually not the case).

**Write a function that, for a fixed group size, randomly draws â€œbirthdaysâ€ for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result.**


```{r write birthday function}
shared_birthdays = function(n) {
  
  random_birthdays = sample(1:365, size = n, replace = TRUE)
  return(any(duplicated(random_birthdays)))
}
```

Now I'm going to test this function:
```{r test function}

shared_birthdays(40)

```

```{r test function 2}

shared_birthdays(12)

```

Okay, after testing this function, a couple of times it appears that it is working okay. That is, it is able to produce either TRUE or FALSE based on the sample size of n people and if there are any duplicated birthdays in the mix. When I test the function with 40, I get TRUE, and with 12 people I get FALSE. 

**Next, run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs. Make a plot showing the probability as a function of group size, and comment on your results.**

loop function
```{r run the function, echo=TRUE}

results_df = tibble(
  group_size = integer(),
  prob_shared_birthdays = numeric()
)
set.seed
for (i in 2:50) {
   shared_birthdays_loop = replicate(10000, shared_birthdays(i))
   results_df = results_df %>%
    add_row(
      group_size = i,
      prob_shared_birthdays = mean(shared_birthdays_loop)
    )
}

```

check out my table
```{r check out my table}
print(results_df)
```

Plot my results table
```{r plot results table}
ggplot(results_df, aes(x = group_size, y = prob_shared_birthdays)) +
geom_line() +
labs(
  title = "Probability of Shared Birthday In Group Size n",
  x = "Group Size",
  y = "Probability of having a Shared Birthday"
  )
```

This plot confirms what I learned when I tested the function earlier on a smaller sample size like 12 which had a FALSE result (indicating a lower probablity of duplication) vs a larger sample size like 40 which had a TRUE result (indicating a higher probability of duplication) which is what the plot shows. As the group size increases, the probability of people having a shared birthday increases which is why live in a world with a huge sample size (if we could sample the whole world) and a lot of duplicated birthdays across the 365 days we count in Western society. On the other hand in my group of close friends which is less than 10, there are no duplicated birthdays because that probability is lower in a smaller sample. 

## Problem 2

When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected â€“ put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

```{r run t test}

sim_t_test = function(mu, n = 30, sigma = 5, num_sim = 5000) {
  replicate(num_sim, {
  x = rnorm(n, mean = mu, sd = sigma)
    
  t_test = t.test(x, mu = 0)
    
  broom::tidy(t_test)
  }, simplify = FALSE) %>% 
    bind_rows() %>% 
    mutate(true_mu = mu)
}

```

Repeat the above for ðœ‡={1,2,3,4,5,6}
and complete the following:

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of ðœ‡
 on the x axis. Describe the association between effect size and power.
Make a plot showing the average estimate of ðœ‡Ì‚ 
 on the y axis and the true value of ðœ‡
 on the x axis. Make a second plot (or overlay on the first) the average estimate of ðœ‡Ì‚ 
 only in samples for which the null was rejected on the y axis and the true value of ðœ‡
 on the x axis. Is the sample average of ðœ‡Ì‚ 
 across tests for which the null is rejected approximately equal to the true value of ðœ‡
? Why or why not?

```{r repeat for different mu values}
repeat_mu_values = 0:6

repeat_mu_df = map_dfr(repeat_mu_values, ~ sim_t_test(mu = .x))
```

plot showing proportion of times null was rejected (power)
```{r plot null rejection frequency}
mu_hat_df = repeat_mu_df %>%
group_by(true_mu) %>%
  summarize(
    power = mean(p.value < 0.05),
    avg_mu_hat = mean(estimate),
    avg_mu_hat_rejected = mean(estimate[p.value < 0.05])
  )

ggplot(mu_hat_df, aes(x = true_mu, y = power)) +
  geom_line() +
  labs(
  title = "Power of the T-Test by true Î¼",
  x = "True Î¼",
  y = "# of null rejections"
  ) +
  theme_minimal()

```
It appears that as the power (proportion of rejections of the null hypothesis) of this test increases, so does the true value of Î¼. 

Plot 2:
```{r plot 2 }
ggplot(mu_hat_df, aes(x = true_mu)) +
  geom_line(aes(y = avg_mu_hat, color = "All Samples")) +
  geom_line(aes(y = avg_mu_hat_rejected, color = "Rejected Null")) +
  labs(
  title = "Average Estimate of Î¼Ì‚ by True Î¼",
  x = "True Î¼",
  y = "Average Î¼Ì‚") +
  scale_color_manual(
    values = c("All Samples" = "green", "Rejected Null" = "blue"),
    name = "Condition"
  ) +
  theme_minimal()
```

Is the sample average of ðœ‡Ì‚ across tests for which the null is rejected approximately equal to the true value of ðœ‡? Why or why not?

No, it doesn't appear from the graph that the sample average Î¼(hat) across tests for which the null is rejected is approximately equal to the true value of Î¼. Instead, it appears that the sample average of Î¼ (hat) across the power (null rejected) is a bit larger than the true value of Î¼. 


## Problem 3
The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository here. You can read their accompanying article here.

Describe the raw data. Create a city_state variable (e.g. â€œBaltimore, MDâ€) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).

```{r read in my data}
library(tidyverse)
library(broom)

wpost_homicide_df = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

take a look at the data to describe it
```{r glimpse at data}
glimpse(wpost_homicide_df)
```
The raw data has 52,179 rows and 12 variables including victim first and last name, race, age, sex, and then location variables such as city, state, exact location (lat, lon) and disposition which appears to be how the case was handled. There doesn't seem to be a lot of missing data. 

merge variables city and state into one variable
```{r merge city and state}
wpost_homicide_df = wpost_homicide_df %>%
mutate(city_state = str_c(city, ", ", state))
```
summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).
```{r summarize data}

city_summary_df = wpost_homicide_df %>%
  mutate(
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved)
  ) %>%
  ungroup()
```
For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r baltimore prop test}
baltimore_df = city_summary_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_test = prop.test(
  x = baltimore_df$unsolved_homicides,
  n = baltimore_df$total_homicides
)

baltimore_results_df = broom::tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high)
```
Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a â€œtidyâ€ pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r all cities prop test}

city_test_df = city_summary_df %>%
  mutate(
  prop_test = map2(
      unsolved_homicides,
      total_homicides,
      ~ broom::tidy(prop.test(x = .x, n = .y))
    )
  ) %>%
  unnest(prop_test) %>%
  select(city_state, estimate, conf.low, conf.high)

```

Create a plot that shows the estimates and CIs for each city â€“ check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r plotting estimates and CIs per city}

city_test_df = city_test_df %>%
  arrange(desc(estimate)) %>%
  mutate(city_state = fct_reorder(city_state, estimate))

ggplot(city_test_df, aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  theme_minimal()

```

From my plot, it appears that the city with the highest proportion of unsolved murders is Chicago, IL with New Orleans second, and Baltimore third whereas cities like Tulsa AL, Richmond VA, and Charlotte NC have the lowest proportion of unsolved murders. There are most likely a lot of factors at play including structural inequities and city sprawl. 

hello 
