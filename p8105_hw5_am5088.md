p8105_hw5_am5088
================
Anika Mitchell am5088

load my necessary libraries

## Problem 1

Iâ€™ve gone ahead and loaded my necessary libraries and set up plotting
for this problem (tidyverse), and will now start to develop a function
to address the following question:

Suppose you put ğ‘›people in a room, and want to know the probability that
at least two people share a birthday. For simplicity, weâ€™ll assume there
are no leap years (i.e.Â there are only 365 days) and that birthdays are
uniformly distributed over the year (which is actually not the case).

**Write a function that, for a fixed group size, randomly draws
â€œbirthdaysâ€ for each person; checks whether there are duplicate
birthdays in the group; and returns TRUE or FALSE based on the result.**

``` r
shared_birthdays = function(n) {
  
  random_birthdays = sample(1:365, size = n, replace = TRUE)
  return(any(duplicated(random_birthdays)))
}
```

Now Iâ€™m going to test this function:

``` r
shared_birthdays(40)
```

    ## [1] TRUE

``` r
shared_birthdays(12)
```

    ## [1] FALSE

Okay, after testing this function, a couple of times it appears that it
is working okay. That is, it is able to produce either TRUE or FALSE
based on the sample size of n people and if there are any duplicated
birthdays in the mix. When I test the function with 40, I get TRUE, and
with 12 people I get FALSE.

**Next, run this function 10000 times for each group size between 2 and
50. For each group size, compute the probability that at least two
people in the group will share a birthday by averaging across the 10000
simulation runs. Make a plot showing the probability as a function of
group size, and comment on your results.**

loop function

``` r
results_df = tibble(
  group_size = integer(),
  prob_shared_birthdays = numeric()
)
set.seed
```

    ## function (seed, kind = NULL, normal.kind = NULL, sample.kind = NULL) 
    ## {
    ##     kinds <- c("Wichmann-Hill", "Marsaglia-Multicarry", "Super-Duper", 
    ##         "Mersenne-Twister", "Knuth-TAOCP", "user-supplied", "Knuth-TAOCP-2002", 
    ##         "L'Ecuyer-CMRG", "default")
    ##     n.kinds <- c("Buggy Kinderman-Ramage", "Ahrens-Dieter", "Box-Muller", 
    ##         "user-supplied", "Inversion", "Kinderman-Ramage", "default")
    ##     s.kinds <- c("Rounding", "Rejection", "default")
    ##     if (length(kind)) {
    ##         if (!is.character(kind) || length(kind) > 1L) 
    ##             stop("'kind' must be a character string of length 1 (RNG to be used).")
    ##         if (is.na(i.knd <- pmatch(kind, kinds) - 1L)) 
    ##             stop(gettextf("'%s' is not a valid abbreviation of an RNG", 
    ##                 kind), domain = NA)
    ##         if (i.knd == length(kinds) - 1L) 
    ##             i.knd <- -1L
    ##     }
    ##     else i.knd <- NULL
    ##     if (!is.null(normal.kind)) {
    ##         if (!is.character(normal.kind) || length(normal.kind) != 
    ##             1L) 
    ##             stop("'normal.kind' must be a character string of length 1")
    ##         normal.kind <- pmatch(normal.kind, n.kinds) - 1L
    ##         if (is.na(normal.kind)) 
    ##             stop(gettextf("'%s' is not a valid choice", normal.kind), 
    ##                 domain = NA)
    ##         if (normal.kind == 0L) 
    ##             stop("buggy version of Kinderman-Ramage generator is not allowed", 
    ##                 domain = NA)
    ##         if (normal.kind == length(n.kinds) - 1L) 
    ##             normal.kind <- -1L
    ##     }
    ##     if (!is.null(sample.kind)) {
    ##         if (!is.character(sample.kind) || length(sample.kind) != 
    ##             1L) 
    ##             stop("'sample.kind' must be a character string of length 1")
    ##         sample.kind <- pmatch(sample.kind, s.kinds) - 1L
    ##         if (is.na(sample.kind)) 
    ##             stop(gettextf("'%s' is not a valid choice", sample.kind), 
    ##                 domain = NA)
    ##         if (sample.kind == 0L) 
    ##             warning("non-uniform 'Rounding' sampler used", domain = NA)
    ##         if (sample.kind == length(s.kinds) - 1L) 
    ##             sample.kind <- -1L
    ##     }
    ##     .Internal(set.seed(seed, i.knd, normal.kind, sample.kind))
    ## }
    ## <bytecode: 0x115cecfb0>
    ## <environment: namespace:base>

``` r
for (i in 2:50) {
   shared_birthdays_loop = replicate(10000, shared_birthdays(i))
   results_df = results_df %>%
    add_row(
      group_size = i,
      prob_shared_birthdays = mean(shared_birthdays_loop)
    )
}
```

check out my table

``` r
print(results_df)
```

    ## # A tibble: 49 Ã— 2
    ##    group_size prob_shared_birthdays
    ##         <int>                 <dbl>
    ##  1          2                0.0021
    ##  2          3                0.006 
    ##  3          4                0.0168
    ##  4          5                0.0263
    ##  5          6                0.0371
    ##  6          7                0.0541
    ##  7          8                0.0761
    ##  8          9                0.0894
    ##  9         10                0.109 
    ## 10         11                0.137 
    ## # â„¹ 39 more rows

Plot my results table

``` r
ggplot(results_df, aes(x = group_size, y = prob_shared_birthdays)) +
geom_line() +
labs(
  title = "Probability of Shared Birthday In Group Size n",
  x = "Group Size",
  y = "Probability of having a Shared Birthday"
  )
```

<img src="p8105_hw5_am5088_files/figure-gfm/plot results table-1.png" width="90%" />

This plot confirms what I learned when I tested the function earlier on
a smaller sample size like 12 which had a FALSE result (indicating a
lower probablity of duplication) vs a larger sample size like 40 which
had a TRUE result (indicating a higher probability of duplication) which
is what the plot shows. As the group size increases, the probability of
people having a shared birthday increases which is why live in a world
with a huge sample size (if we could sample the whole world) and a lot
of duplicated birthdays across the 365 days we count in Western society.
On the other hand in my group of close friends which is less than 10,
there are no duplicated birthdays because that probability is lower in a
smaller sample.

## Problem 2

When designing an experiment or analysis, a common question is whether
it is likely that a true effect will be detected â€“ put differently,
whether a false null hypothesis will be rejected. The probability that a
false null hypothesis is rejected is referred to as power, and it
depends on several factors, including: the sample size; the effect size;
and the error variance. In this problem, you will conduct a simulation
to explore power in a one-sample t-test.

``` r
sim_t_test = function(mu, n = 30, sigma = 5, num_sim = 5000) {
  replicate(num_sim, {
  x = rnorm(n, mean = mu, sd = sigma)
    
  t_test = t.test(x, mu = 0)
    
  broom::tidy(t_test)
  }, simplify = FALSE) %>% 
    bind_rows() %>% 
    mutate(true_mu = mu)
}
```

Repeat the above for ğœ‡={1,2,3,4,5,6} and complete the following:

Make a plot showing the proportion of times the null was rejected (the
power of the test) on the y axis and the true value of ğœ‡ on the x axis.
Describe the association between effect size and power. Make a plot
showing the average estimate of ğœ‡Ì‚ on the y axis and the true value of ğœ‡
on the x axis. Make a second plot (or overlay on the first) the average
estimate of ğœ‡Ì‚ only in samples for which the null was rejected on the y
axis and the true value of ğœ‡ on the x axis. Is the sample average of ğœ‡Ì‚
across tests for which the null is rejected approximately equal to the
true value of ğœ‡ ? Why or why not?

``` r
repeat_mu_values = 0:6

repeat_mu_df = map_dfr(repeat_mu_values, ~ sim_t_test(mu = .x))
```

plot showing proportion of times null was rejected (power)

``` r
mu_hat_df = repeat_mu_df %>%
group_by(true_mu) %>%
  summarize(
    power = mean(p.value < 0.05),
    avg_mu_hat = mean(estimate),
    avg_mu_hat_rejected = mean(estimate[p.value < 0.05])
  )

ggplot(mu_hat_df, aes(x = true_mu, y = power)) +
  geom_line() +
  labs(
  title = "Power of the T-Test by true Î¼",
  x = "True Î¼",
  y = "# of null rejections"
  ) +
  theme_minimal()
```

<img src="p8105_hw5_am5088_files/figure-gfm/plot null rejection frequency-1.png" width="90%" />
It appears that as the power (proportion of rejections of the null
hypothesis) of this test increases, so does the true value of Î¼.

Plot 2:

``` r
ggplot(mu_hat_df, aes(x = true_mu)) +
  geom_line(aes(y = avg_mu_hat, color = "All Samples")) +
  geom_line(aes(y = avg_mu_hat_rejected, color = "Rejected Null")) +
  labs(
  title = "Average Estimate of Î¼Ì‚ by True Î¼",
  x = "True Î¼",
  y = "Average Î¼Ì‚") +
  scale_color_manual(
    values = c("All Samples" = "green", "Rejected Null" = "blue"),
    name = "Condition"
  ) +
  theme_minimal()
```

<img src="p8105_hw5_am5088_files/figure-gfm/plot 2-1.png" width="90%" />

Is the sample average of ğœ‡Ì‚ across tests for which the null is rejected
approximately equal to the true value of ğœ‡? Why or why not?

No, it doesnâ€™t appear from the graph that the sample average Î¼(hat)
across tests for which the null is rejected is approximately equal to
the true value of Î¼. Instead, it appears that the sample average of Î¼
(hat) across the power (null rejected) is a bit larger than the true
value of Î¼.

## Problem 3

The Washington Post has gathered data on homicides in 50 large U.S.
cities and made the data available through a GitHub repository here. You
can read their accompanying article here.

Describe the raw data. Create a city_state variable (e.g.Â â€œBaltimore,
MDâ€) and then summarize within cities to obtain the total number of
homicides and the number of unsolved homicides (those for which the
disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).

``` r
library(tidyverse)
library(broom)

wpost_homicide_df = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

take a look at the data to describe it

``` r
glimpse(wpost_homicide_df)
```

    ## Rows: 52,179
    ## Columns: 12
    ## $ uid           <chr> "Alb-000001", "Alb-000002", "Alb-000003", "Alb-000004", â€¦
    ## $ reported_date <dbl> 20100504, 20100216, 20100601, 20100101, 20100102, 201001â€¦
    ## $ victim_last   <chr> "GARCIA", "MONTOYA", "SATTERFIELD", "MENDIOLA", "MULA", â€¦
    ## $ victim_first  <chr> "JUAN", "CAMERON", "VIVIANA", "CARLOS", "VIVIAN", "GERALâ€¦
    ## $ victim_race   <chr> "Hispanic", "Hispanic", "White", "Hispanic", "White", "Wâ€¦
    ## $ victim_age    <chr> "78", "17", "15", "32", "72", "91", "52", "52", "56", "4â€¦
    ## $ victim_sex    <chr> "Male", "Male", "Female", "Male", "Female", "Female", "Mâ€¦
    ## $ city          <chr> "Albuquerque", "Albuquerque", "Albuquerque", "Albuquerquâ€¦
    ## $ state         <chr> "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "Nâ€¦
    ## $ lat           <dbl> 35.09579, 35.05681, 35.08609, 35.07849, 35.13036, 35.151â€¦
    ## $ lon           <dbl> -106.5386, -106.7153, -106.6956, -106.5561, -106.5810, -â€¦
    ## $ disposition   <chr> "Closed without arrest", "Closed by arrest", "Closed witâ€¦

The raw data has 52,179 rows and 12 variables including victim first and
last name, race, age, sex, and then location variables such as city,
state, exact location (lat, lon) and disposition which appears to be how
the case was handled. There doesnâ€™t seem to be a lot of missing data.

merge variables city and state into one variable

``` r
wpost_homicide_df = wpost_homicide_df %>%
mutate(city_state = str_c(city, ", ", state))
```

summarize within cities to obtain the total number of homicides and the
number of unsolved homicides (those for which the disposition is â€œClosed
without arrestâ€ or â€œOpen/No arrestâ€).

``` r
city_summary_df = wpost_homicide_df %>%
  mutate(
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved)
  ) %>%
  ungroup()
```

For the city of Baltimore, MD, use the prop.test function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.

``` r
baltimore_df = city_summary_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_test = prop.test(
  x = baltimore_df$unsolved_homicides,
  n = baltimore_df$total_homicides
)

baltimore_results_df = broom::tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high)
```

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a â€œtidyâ€ pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
city_test_df = city_summary_df %>%
  mutate(
  prop_test = map2(
      unsolved_homicides,
      total_homicides,
      ~ broom::tidy(prop.test(x = .x, n = .y))
    )
  ) %>%
  unnest(prop_test) %>%
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## â„¹ In argument: `prop_test = map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

Create a plot that shows the estimates and CIs for each city â€“ check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

``` r
city_test_df = city_test_df %>%
  arrange(desc(estimate)) %>%
  mutate(city_state = fct_reorder(city_state, estimate))

ggplot(city_test_df, aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  theme_minimal()
```

<img src="p8105_hw5_am5088_files/figure-gfm/plotting estimates and CIs per city-1.png" width="90%" />
From my plot, it appears that the city with the highest proportion of
unsolved murders is Chicago, IL with New Orleans second, and Baltimore
third whereas cities like Tulsa AL, Richmond VA, and Charlotte NC have
the lowest proportion of unsolved murders. There are most likely a lot
of factors at play including structural inequities and city sprawl.
